{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcee9e5b-4097-4124-b03f-601a7a0825ff",
   "metadata": {},
   "source": [
    "# 9. Data pipeline extracting data from CSV, building datasets and storing them in parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e849f1-e589-48ef-807c-698f35c34ebd",
   "metadata": {},
   "source": [
    "---\n",
    "**To be able to execute this notebook, please download your own local version of the data from https://www.kaggle.com/martinellis/nhl-game-data and store it into the folder \"data/nhl/nhl_stats/\"**\n",
    "\n",
    "\n",
    "\n",
    "**In addition, please download your own local version of the data from https://www.kaggle.com/camnugent/predict-nhl-player-salaries and store it into the folder \"data/nhl/nhl_salaries/\"**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cdef30-f5a1-445c-a66d-14dc59185904",
   "metadata": {},
   "source": [
    "This notebook provides a data pipeline to extract data from divers CSV files, merge it and model new features out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d668a5e8-c6b9-4ae7-a5b9-8229402919e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "from utils import create_dummy_df\n",
    "from utils import balance_binary_target\n",
    "\n",
    "from models import run_logistic_regression\n",
    "from models import run_lightgbm_classifier\n",
    "from models import run_kneighbors_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68259748-17b9-4843-b574-caeabff49f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(df): \n",
    "    '''\n",
    "    Function calculates distance to the goal based on the location\n",
    "    INPUT:\n",
    "    df - data frame with location\n",
    "    \n",
    "    OUTPUT:\n",
    "    distance to the goal\n",
    "    ''' \n",
    "    middle_goal_x = 100 \n",
    "    middle_goal_y = 0\n",
    "    return math.hypot(middle_goal_x - df.st_x, middle_goal_y - df.st_y)\n",
    "\n",
    "\n",
    "def angle(df):   \n",
    "    '''\n",
    "    Function calculated angle of the shot measured to the longitude axis of the hockey field \n",
    "    INPUT:\n",
    "    df - data frame with location\n",
    "    \n",
    "    OUTPUT:\n",
    "    angle of the shot\n",
    "    '''    \n",
    "    middle_goal_x = 100 \n",
    "    return math.fabs(math.degrees(math.atan2(df.st_x - middle_goal_x, df.st_y)) + 90)\n",
    "\n",
    "\n",
    "    \n",
    "def extract_dataset(force=False):\n",
    "    '''\n",
    "    Function extract all data from csv files and store experimental datasets as parquet files \n",
    "    INPUT:\n",
    "    force - flag if the data should be extract and replaced even if the parquet files do exist\n",
    "        \n",
    "    OUTPUT: no\n",
    "    '''       \n",
    "    \n",
    "    # if not forced and all parquet files do exist, skip data extraction\n",
    "    if ((force==False) and \n",
    "        (os.path.isfile('data/_1_first_dataset.parquet') and\n",
    "        os.path.isfile('data/_2_balanced_target.parquet') and \n",
    "        os.path.isfile('data/_3_with_distance.parquet') and \n",
    "        os.path.isfile('data/_4_with_distance_angle.parquet') and \n",
    "        os.path.isfile('data/_5_with_player_ids.parquet') and \n",
    "        os.path.isfile('data/_6_with_player_stats.parquet') and \n",
    "        os.path.isfile('data/_7_with_player_salary.parquet') and \n",
    "        os.path.isfile('data/_8_short_dist.parquet') and \n",
    "        os.path.isfile('data/_9_long_dist.parquet'))):\n",
    "        print('Data extraction skipped')\n",
    "        return \n",
    "    \n",
    "    print('Started data extraction...')\n",
    "    \n",
    "    # read data\n",
    "    df_game_plays = pd.read_csv('data/nhl/nhl_stats/game_plays.csv')\n",
    "    df_game_plays.drop_duplicates(inplace=True)\n",
    "\n",
    "    # drop NaNs, select relevant columns only\n",
    "    df = df_game_plays[['play_id', 'event', 'secondaryType', 'st_x', 'st_y', 'period', 'periodTime']][(df_game_plays.event=='Goal') | (df_game_plays.event=='Shot')]\n",
    "    df.dropna(inplace=True)\n",
    "    df.secondaryType = df.secondaryType.str.replace(\" \", \"\")\n",
    "    df.secondaryType = df.secondaryType.str.replace(\"-\", \"\")\n",
    "\n",
    "    # Prepare target. Convert categorical event values 'Goal' to a numerical 0/1 value indicating goal\n",
    "    df['goal'] = np.where(df.event=='Goal', 1, 0)\n",
    "    df.drop(columns='event', inplace=True)\n",
    "\n",
    "    # Convert categorical 'secondaryType' column to multiple numerical columns\n",
    "    cat_cols = ['secondaryType']\n",
    "    df = create_dummy_df(df=df, cat_cols=cat_cols, dummy_na=True)\n",
    "    df.drop(columns=['secondaryType_nan'], inplace=True)\n",
    "\n",
    "    #####################\n",
    "    df.to_parquet('data/_1_first_dataset.parquet', compression='brotli')\n",
    "    #####################\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    #balance targets\n",
    "    df = balance_binary_target(df, target='goal')\n",
    "\n",
    "    #####################\n",
    "    df.to_parquet('data/_2_balanced_target.parquet', compression='brotli')\n",
    "    #####################\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # adding feature distance\n",
    "    df['distance'] = df.apply(dist, axis=1)\n",
    "\n",
    "    # cut outliers first\n",
    "    df = df[df.distance<100]\n",
    "\n",
    "    #####################\n",
    "    df.to_parquet('data/_3_with_distance.parquet', compression='brotli')\n",
    "    #####################\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # adding feature angle\n",
    "    df['angle'] = df.apply(angle, axis=1)\n",
    "\n",
    "    # cutting outliers\n",
    "    df= df[df.angle<70]\n",
    "\n",
    "    #####################\n",
    "    df.to_parquet('data/_4_with_distance_angle.parquet', compression='brotli')\n",
    "    #####################\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # adding players to the game\n",
    "    df_player = pd.read_csv('data/nhl/nhl_stats/game_plays_players.csv')\n",
    "    df_player.drop_duplicates(inplace=True)\n",
    "\n",
    "    #skater\n",
    "    df_skater = df_player[df_player.playerType.isin(['Shooter', 'Scorer'])].copy()\n",
    "    df_skater.rename(columns={'player_id':'skater_id'}, inplace=True)\n",
    "    df_skater.drop(columns=['game_id', 'playerType'], inplace=True)\n",
    "\n",
    "    #goalies\n",
    "    df_goalie = df_player[df_player.playerType.isin(['Goalie'])].copy()\n",
    "    df_goalie.rename(columns={'player_id':'goalie_id'}, inplace=True)\n",
    "    df_goalie.drop(columns=['game_id', 'playerType'], inplace=True)\n",
    "\n",
    "    # created new table and merge skaters and golies to the game events\n",
    "    df = df.merge(df_skater, how='left', on='play_id')\n",
    "    df = df.merge(df_goalie, how='left', on='play_id')\n",
    "\n",
    "    # Apparently not all shots were on goal. No golie then. Keeping this rows anyway but replacing goalie by Zero\n",
    "    df.goalie_id.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "    #####################\n",
    "    df.to_parquet('data/_5_with_player_ids.parquet', compression='brotli')\n",
    "    #####################\n",
    "\n",
    "    ###############################################################\n",
    "    # adding player statistics\n",
    "    df_player_info = pd.read_csv('data/nhl/nhl_stats/player_info.csv')\n",
    "\n",
    "    # calculating overall savePercentage for each goalie\n",
    "    df_goalie = pd.read_csv('data/nhl/nhl_stats/game_goalie_stats.csv')\n",
    "    df_goalie = df_goalie.groupby('player_id').agg({'savePercentage':'mean'}).reset_index()\n",
    "    df_goalie.rename(columns={'player_id':'goalie_id'}, inplace=True)\n",
    "\n",
    "    # calculating overall statistics for each skater\n",
    "    df_skater = pd.read_csv('data/nhl/nhl_stats/game_skater_stats.csv')\n",
    "    df_skater = df_skater.groupby('player_id').agg({'goals':'sum', 'shots':'sum', 'assists':'sum', 'timeOnIce':'sum'}).reset_index()\n",
    "    df_skater.rename(columns={'player_id':'skater_id'}, inplace=True)\n",
    "\n",
    "    # merging it together in the training dataset\n",
    "    df = df.merge(df_skater, how='left', on='skater_id')\n",
    "    df = df.merge(df_goalie, how='left', on='goalie_id')\n",
    "\n",
    "    # filling savePercentage\n",
    "    df.savePercentage.fillna(df.savePercentage.median(), inplace=True)\n",
    "    # drop all remaining rows with NaNs\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    #####################\n",
    "    df.to_parquet('data/_6_with_player_stats.parquet', compression='BROTLI')\n",
    "    #####################\n",
    "\n",
    "    ###############################################################\n",
    "    # adding player salaries\n",
    "    # Data salaries from another dataset\n",
    "    df_players_train = pd.read_csv('data/nhl/nhl_salaries/train.csv')\n",
    "    df_players_test = pd.read_csv('data/nhl/nhl_salaries/test.csv')\n",
    "    df_player_stats = pd.concat([df_players_train, df_players_test])\n",
    "\n",
    "    df_player_stats = df_player_stats[['First Name', 'Last Name', 'Nat', 'Salary']].rename(columns={'First Name':'firstName', 'Last Name':'lastName', 'Nat':'nationality', 'Salary':'salary'})\n",
    "\n",
    "    # prepare player_info for merging\n",
    "    df_player_info.rename(columns={'player_id':'skater_id'}, inplace=True)\n",
    "\n",
    "    #add name and nationality of the skater to the main dataset\n",
    "    df = df.merge(df_player_info[['skater_id', 'firstName', 'lastName', 'nationality']], how='left', on='skater_id')\n",
    "\n",
    "    # add salary based on first name, last name, and the nationality\n",
    "    df = df.merge(df_player_stats, how='left', on=['firstName', 'lastName', 'nationality'])\n",
    "\n",
    "    # replace missing salary\n",
    "    df.salary.fillna(df.salary.median(), inplace=True)\n",
    "    df.savePercentage.fillna(df.savePercentage.median(), inplace=True)\n",
    "\n",
    "    # cut off outliers\n",
    "    df = df[df.goals<400]\n",
    "    df = df[df.shots<4000]\n",
    "    df = df[df.assists<700]\n",
    "    df = df[df.salary<10000000]\n",
    "\n",
    "    #####################\n",
    "    df.to_parquet('data/_7_with_player_salary.parquet', compression='BROTLI')\n",
    "    #####################\n",
    "\n",
    "    ###############################################################\n",
    "    # selecting short distance shots\n",
    "    df_short_dist = df[df.distance<30]\n",
    "\n",
    "    #####################\n",
    "    df_short_dist.to_parquet('data/_8_short_dist.parquet', compression='BROTLI')\n",
    "    #####################\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # selecting long distance shots\n",
    "    df_long_dist = df[df.distance>=30]\n",
    "\n",
    "    #####################\n",
    "    df_long_dist.to_parquet('data/_9_long_dist.parquet', compression='BROTLI')\n",
    "    #####################\n",
    "    \n",
    "    print('Data extraction done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18149d-552a-4dca-b247-aef969004247",
   "metadata": {},
   "source": [
    "---\n",
    "### Extract datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03e21ce-d2c1-44eb-b02b-fe50a566f696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started data extraction...\n",
      "Data extraction done!\n"
     ]
    }
   ],
   "source": [
    "extract_dataset(force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e19fe6-f82a-4445-8016-93cc8c103c66",
   "metadata": {},
   "source": [
    "---\n",
    "Testing dataset in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946f293a-177f-4775-8887-973986e48888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/_6_with_player_stats.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56bff6e6-0ccb-4269-be8d-a46e62f78237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Logistic Regression ==========\n",
      "Accuracy: 0.5516\n",
      "F1 Score: 0.4954\n",
      "AUC-ROC Score: 0.5870\n",
      "-----------------------------------------\n",
      "Precision: 0.5687\n",
      "Recall: 0.4389\n",
      "=========================================\n",
      "========== LightGBM Classifier ==========\n",
      "Accuracy: 0.6974\n",
      "F1 Score: 0.7119\n",
      "AUC-ROC Score: 0.7595\n",
      "-----------------------------------------\n",
      "Precision: 0.6812\n",
      "Recall: 0.7455\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "# starting training \n",
    "run_logistic_regression(df, c_matrix=False, r_curve=False);\n",
    "run_lightgbm_classifier(df, c_matrix=False, r_curve=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
